{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000022FF06AD8C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000022FF06AD8C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000022FF06AD8C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000022FF06AD8C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000022FF0884A08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000022FF0884A08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000022FF0884A08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000022FF0884A08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "#encoding\n",
    "#0,1,2\n",
    "#[S,E,P.......]\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀연습사랑'] \n",
    "#                       0,1,2\n",
    "#범주형 데이터\n",
    "num_dic={n: i for i, n in enumerate(char_arr)}\n",
    "dic_len=len(num_dic)\n",
    "print(dic_len) #41\n",
    "seq_data=[['word','단어'],['wood','나무'],\n",
    "          ['game','놀이'],['girl','소녀'],\n",
    "          ['test','연습'],['love','사랑']]\n",
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch=[]\n",
    "    output_batch=[]\n",
    "    target_batch=[]\n",
    "    for seq in seq_data: #처음 ['word','단어'] \n",
    "          input=[num_dic[n] for n in seq[0]] #'word'  =>번호 w: 25, o :20 ,r :17...\n",
    "          #첫번째 입력되는 데이터는 'word' 를 학습해서 넘어온 smoking gun \n",
    "          output=[num_dic[n] for n in ('S' +seq[1])] #훈련된 특징값,'S' 시작점 \n",
    "          target=[num_dic[n] for n in (seq[1]+'E')] #끝을 확인 하기 위해서 '연습이다'-> '단어EE'\n",
    "          #identity matrix 단위행렬=>one hot encoding\n",
    "          #1000000000\n",
    "          #0100000000\n",
    "          # ........\n",
    "          input_batch.append(np.eye(dic_len)[input]) #번호로 만든것 예)25가 들어가면 2차원 배열로 4 x 42 \n",
    "          output_batch.append(np.eye(dic_len)[output])\n",
    "          target_batch.append(target)                #2x 41\n",
    "    return input_batch,output_batch,target_batch\n",
    "learning_rate=0.01\n",
    "n_hidden=128   #neurons 출력이 128차\n",
    "total_epoch=100\n",
    "n_class=n_input=dic_len #41\n",
    "#encoder,decoder  용 데이터 주입변수\n",
    "enc_input=tf.placeholder(tf.float32,[None,None,n_input])\n",
    "dec_input=tf.placeholder(tf.float32,[None,None,n_input])\n",
    "targets=tf.placeholder(tf.int64,[None,None])\n",
    "\n",
    "with tf.variable_scope('encode'): #input4개 \n",
    "    #셀 자체는 FFNN 한 개 , 입력->가중치->>출력(n_hidden)\n",
    "    enc_cell=tf.nn.rnn_cell.BasicRNNCell(n_hidden) #입력 41 x 128\n",
    "    #과적합방지하기 위해 계산회로 생략-random 하게 생략\n",
    "    enc_cell=tf.nn.rnn_cell.DropoutWrapper(enc_cell,output_keep_prob=0.5,seed=100) \n",
    "    #셀이 4개 셀당 41개의 데이터가 입력\n",
    "    outputs,enc_states=tf.nn.dynamic_rnn(enc_cell,enc_input,dtype=tf.float32)\n",
    "with tf.variable_scope('decode'): #outputs 3개 \n",
    "    dec_cell=tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell=tf.nn.rnn_cell.DropoutWrapper(dec_cell,output_keep_prob=0.5,seed=100)\n",
    "   #2 x 41??\n",
    "    #앞의 인코더 망에서 출력된 states값:마지막 cell 의 출력값\n",
    "    #앞의 데이터를 다 고려한 특징 \n",
    "    #두개의 망을 연결 initial_state=enc_states\n",
    "    outputs,enc_states=tf.nn.dynamic_rnn(dec_cell,dec_input,initial_state=enc_states,dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000022FEDA677C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000022FEDA677C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000022FEDA677C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000022FEDA677C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "#전체 데이터가 6개, (output)3개!!가 128개로 나온다 :'S'+'단어'=3개\n",
    "#(6x3x128 ) x 41 행렬 곱하면 \n",
    "#6x3x41\n",
    "\n",
    "model=tf.layers.dense(outputs,n_class,activation=None)\n",
    "cost=tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model,labels=targets))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0001 cost= 2.693845\n",
      "epoch: 0002 cost= 1.847687\n",
      "epoch: 0003 cost= 1.406774\n",
      "epoch: 0004 cost= 0.630577\n",
      "epoch: 0005 cost= 0.404428\n",
      "epoch: 0006 cost= 0.364389\n",
      "epoch: 0007 cost= 0.223047\n",
      "epoch: 0008 cost= 0.100556\n",
      "epoch: 0009 cost= 0.095548\n",
      "epoch: 0010 cost= 0.082468\n",
      "epoch: 0011 cost= 0.046243\n",
      "epoch: 0012 cost= 0.065438\n",
      "epoch: 0013 cost= 0.020039\n",
      "epoch: 0014 cost= 0.023188\n",
      "epoch: 0015 cost= 0.015581\n",
      "epoch: 0016 cost= 0.016316\n",
      "epoch: 0017 cost= 0.023700\n",
      "epoch: 0018 cost= 0.014285\n",
      "epoch: 0019 cost= 0.006344\n",
      "epoch: 0020 cost= 0.013925\n",
      "epoch: 0021 cost= 0.010044\n",
      "epoch: 0022 cost= 0.004519\n",
      "epoch: 0023 cost= 0.006793\n",
      "epoch: 0024 cost= 0.008879\n",
      "epoch: 0025 cost= 0.000822\n",
      "epoch: 0026 cost= 0.002353\n",
      "epoch: 0027 cost= 0.005951\n",
      "epoch: 0028 cost= 0.002346\n",
      "epoch: 0029 cost= 0.001693\n",
      "epoch: 0030 cost= 0.002250\n",
      "epoch: 0031 cost= 0.002017\n",
      "epoch: 0032 cost= 0.014444\n",
      "epoch: 0033 cost= 0.015728\n",
      "epoch: 0034 cost= 0.001820\n",
      "epoch: 0035 cost= 0.001198\n",
      "epoch: 0036 cost= 0.001243\n",
      "epoch: 0037 cost= 0.001254\n",
      "epoch: 0038 cost= 0.000681\n",
      "epoch: 0039 cost= 0.000806\n",
      "epoch: 0040 cost= 0.000550\n",
      "epoch: 0041 cost= 0.001474\n",
      "epoch: 0042 cost= 0.000552\n",
      "epoch: 0043 cost= 0.000455\n",
      "epoch: 0044 cost= 0.000389\n",
      "epoch: 0045 cost= 0.002738\n",
      "epoch: 0046 cost= 0.000773\n",
      "epoch: 0047 cost= 0.002579\n",
      "epoch: 0048 cost= 0.002015\n",
      "epoch: 0049 cost= 0.000689\n",
      "epoch: 0050 cost= 0.000612\n",
      "epoch: 0051 cost= 0.000839\n",
      "epoch: 0052 cost= 0.001440\n",
      "epoch: 0053 cost= 0.001147\n",
      "epoch: 0054 cost= 0.000181\n",
      "epoch: 0055 cost= 0.000526\n",
      "epoch: 0056 cost= 0.000324\n",
      "epoch: 0057 cost= 0.000353\n",
      "epoch: 0058 cost= 0.001034\n",
      "epoch: 0059 cost= 0.000559\n",
      "epoch: 0060 cost= 0.000667\n",
      "epoch: 0061 cost= 0.001083\n",
      "epoch: 0062 cost= 0.000556\n",
      "epoch: 0063 cost= 0.000576\n",
      "epoch: 0064 cost= 0.000657\n",
      "epoch: 0065 cost= 0.001080\n",
      "epoch: 0066 cost= 0.000176\n",
      "epoch: 0067 cost= 0.000285\n",
      "epoch: 0068 cost= 0.000767\n",
      "epoch: 0069 cost= 0.000381\n",
      "epoch: 0070 cost= 0.000594\n",
      "epoch: 0071 cost= 0.000233\n",
      "epoch: 0072 cost= 0.000469\n",
      "epoch: 0073 cost= 0.000212\n",
      "epoch: 0074 cost= 0.000820\n",
      "epoch: 0075 cost= 0.000184\n",
      "epoch: 0076 cost= 0.000248\n",
      "epoch: 0077 cost= 0.000291\n",
      "epoch: 0078 cost= 0.000749\n",
      "epoch: 0079 cost= 0.000349\n",
      "epoch: 0080 cost= 0.000187\n",
      "epoch: 0081 cost= 0.000384\n",
      "epoch: 0082 cost= 0.000201\n",
      "epoch: 0083 cost= 0.000989\n",
      "epoch: 0084 cost= 0.000163\n",
      "epoch: 0085 cost= 0.000159\n",
      "epoch: 0086 cost= 0.000302\n",
      "epoch: 0087 cost= 0.000127\n",
      "epoch: 0088 cost= 0.000334\n",
      "epoch: 0089 cost= 0.000512\n",
      "epoch: 0090 cost= 0.000423\n",
      "epoch: 0091 cost= 0.000283\n",
      "epoch: 0092 cost= 0.000318\n",
      "epoch: 0093 cost= 0.000181\n",
      "epoch: 0094 cost= 0.000278\n",
      "epoch: 0095 cost= 0.000424\n",
      "epoch: 0096 cost= 0.000371\n",
      "epoch: 0097 cost= 0.000477\n",
      "epoch: 0098 cost= 0.000214\n",
      "epoch: 0099 cost= 0.000340\n",
      "epoch: 0100 cost= 0.000374\n"
     ]
    }
   ],
   "source": [
    "input_batch,output_batch,target_batch=make_batch(seq_data)\n",
    "for epoch in range(total_epoch): #100회\n",
    "    _,loss=sess.run([optimizer,cost],feed_dict={enc_input:input_batch,\n",
    "                                               dec_input:output_batch,\n",
    "                                               targets:target_batch})\n",
    "    print('epoch:','%04d' % (epoch + 1),'cost=','{:.6f}'.format(loss)) #epoch + 1 처음이 0부터 시작하기 때문에 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word-> 단어\n"
     ]
    }
   ],
   "source": [
    "def translate(word): #translate =함수\n",
    "    #모델의 자리수는 일치시켜야 함 그래서 PPPP\n",
    "    seq_data=[word,'P' * len(word)] #word, PPPP\n",
    "    #1x4x41\n",
    "    #1x4x41 (원핫인코딩)\n",
    "    input_batch,output_batch,target_batch=make_batch([seq_data])\n",
    "    \n",
    "    #1x5x41 : (PPPP+E =5)\n",
    "    prediction=tf.argmax(model,2) #면, 행, 렬 중 열을 중심으로 (41개중 max)\n",
    "    result=sess.run(prediction,feed_dict={enc_input: input_batch,\n",
    "                                         dec_input:output_batch,\n",
    "                                         targets:target_batch})\n",
    "    decoded=[char_arr[i] for i in result[0]] #5 x 41\n",
    "    end=decoded.index('E')\n",
    "    translated=''.join(decoded[:end])\n",
    "    \n",
    "    return translated\n",
    "print('word->',translate('word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OCR: kakao API\n",
    "#카카오 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2 #computer vision 이미지, 동영상처리\n",
    "import requests #html 페이지를 요청\n",
    "import sys\n",
    "LIMIT_PX=1024\n",
    "LIMIT_BYTE=1024*1024\n",
    "LIMIT_BOX=40\n",
    "\n",
    "#ocr 절차\n",
    "#이미지사이즈 조정->글씨를 둘러싼 box를 detection (컬러를 찾아서 박스로 잘라냄)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kaka0_ocr_resize(image_path:str): #규격이미지 사이즈\n",
    "    image=cv2.imread('2.jpg') #자기의 이미지로 변경:영수증\n",
    "    height,width,_=image.shape\n",
    "    if LIMIT_PX < height or LIMIT_PX < width:\n",
    "        ratio=float(LIMIT_PX) / max(height,width)\n",
    "        image=cv2.resize(image,None,fx=ratio,fy=ratio)\n",
    "        height,width,_=height,width,_=image.shape\n",
    "        image_path='{}_resized.jpg'.format('2')\n",
    "        cv2.imwrite(image_path,image)\n",
    "        return image_path\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#카카오에서 준 아이디를 기입\n",
    "인코딩할때 jpg 로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용법\n",
    "#open api 함수를 원격호출(remote call)->네트워크를 통해서 requests 호출(시간이 걸림: 비동기방식\n",
    "#비동기방식:호출시키고 돌아올때까지 다른 작업을 하고 있음- event를 받아서 \n",
    "#대표적인 비동기 통신방식: AJAX :웹브라우저 내부적 처리\n",
    "\n",
    "def kakao_ocr_detect(image_path:str,appkey:str):\n",
    "    API_URL='https://kapi.kakao.com/v1/vision/text/detect'\n",
    "    headers={'authorization':'KakaoAK{}'.format('자신의 API_ID')}\n",
    "    #naver=>jpg 혁식만, gif(애니메이션), png(transparent 지원)\n",
    "    image=cv2.imread('2.jpg') #cv2 통해서 이미지를 로드할때\n",
    "    jpeg_image=cv2.imecode('.jpg',image)[1]\n",
    "    data=jpeg_image.tobytes() #네크워크 전송->serialization\n",
    "    \n",
    "    #네트워크일때->데이터가시리얼로 가야함(네크워크모델)\n",
    "    #8개 line 으로 구성=>4개 회선만 사용->1개 전송 ,1개는 수신\n",
    "    #flask에서 사용\n",
    "    #인서텟에서 표준 데이터 포멧:json \n",
    "    return requests.post(API_URL,headers=headers,files={'file':data},data={'boxes':json.dumps(boxes)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kakao_ocr_resize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-c6db48129afe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mboxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLIMIT_BOX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkakao_ocr_recognize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mappkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-c6db48129afe>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mappkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mresize_impath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkakao_ocr_resize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkakao_ocr_detect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mappkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mboxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'boxes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kakao_ocr_resize' is not defined"
     ]
    }
   ],
   "source": [
    "#자동 가계부\n",
    "def main():\n",
    "    image_path,appkey=sys.argv[0],sys.argv[0]\n",
    "    resize_impath=kakao_ocr_resize(image_path)\n",
    "    output=kakao_ocr_detect(image_path,appkey).json()\n",
    "    boxes=output['result']['boxes']\n",
    "    boxes=boxes[:min(len(boxes),LIMIT_BOX)]\n",
    "    output=kakao_ocr_recognize(image_path,boxes,appkey).json()\n",
    "output=main()\n",
    "output['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import cv2\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "LIMIT_PX = 1024\n",
    "LIMIT_BYTE = 1024*1024  # 1MB\n",
    "LIMIT_BOX = 40\n",
    "\n",
    "\n",
    "def kakao_ocr_resize(image_path: str):\n",
    "    \"\"\"\n",
    "    ocr detect/recognize api helper\n",
    "    ocr api의 제약사항이 넘어서는 이미지는 요청 이전에 전처리가 필요.\n",
    "\n",
    "    pixel 제약사항 초과: resize\n",
    "    용량 제약사항 초과  : 다른 포맷으로 압축, 이미지 분할 등의 처리 필요. (예제에서 제공하지 않음)\n",
    "\n",
    "    :param image_path: 이미지파일 경로\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    if LIMIT_PX < height or LIMIT_PX < width:\n",
    "        ratio = float(LIMIT_PX) / max(height, width)\n",
    "        image = cv2.resize(image, None, fx=ratio, fy=ratio)\n",
    "        height, width, _ = height, width, _ = image.shape\n",
    "\n",
    "        # api 사용전에 이미지가 resize된 경우, recognize시 resize된 결과를 사용해야함.\n",
    "        image_path = \"{}_resized.jpg\".format(image_path)\n",
    "        cv2.imwrite(image_path, image)\n",
    "\n",
    "        return image_path\n",
    "    return None\n",
    "\n",
    "\n",
    "def kakao_ocr_detect(image_path: str, appkey: str):\n",
    "    \"\"\"\n",
    "    detect api request example\n",
    "    :param image_path: 이미지파일 경로\n",
    "    :param appkey: 카카오 앱 REST API 키\n",
    "    \"\"\"\n",
    "    API_URL = 'https://kapi.kakao.com/v1/vision/text/detect'\n",
    "\n",
    "    headers = {'Authorization': 'KakaoAK {}'.format(appkey)}\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    jpeg_image = cv2.imencode(\".jpg\", image)[1]\n",
    "    data = jpeg_image.tobytes()\n",
    "\n",
    "    return requests.post(API_URL, headers=headers, files={\"file\": data})\n",
    "\n",
    "\n",
    "def kakao_ocr_recognize(image_path: str, boxes: list, appkey: str):\n",
    "    \"\"\"\n",
    "    recognize api request example\n",
    "    :param boxes: 감지된 영역 리스트. Canvas 좌표계: 좌상단이 (0,0) / 우상단이 (limit,0)\n",
    "                    감지된 영역중 좌상단 점을 기준으로 시계방향 순서, 좌상->우상->우하->좌하\n",
    "                    ex) [[[0,0],[1,0],[1,1],[0,1]], [[1,1],[2,1],[2,2],[1,2]], ...]\n",
    "    :param image_path: 이미지 파일 경로\n",
    "    :param appkey: 카카오 앱 REST API 키\n",
    "    \"\"\"\n",
    "    API_URL = 'https://kapi.kakao.com/v1/vision/text/recognize'\n",
    "\n",
    "    headers = {'Authorization': 'KakaoAK {}'.format(appkey)}\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    jpeg_image = cv2.imencode(\".jpg\", image)[1]\n",
    "    data = jpeg_image.tobytes()\n",
    "\n",
    "    return requests.post(API_URL, headers=headers, files={\"file\": data}, data={\"boxes\": json.dumps(boxes)})\n",
    "\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Please run with args: $ python example.py /path/to/image appkey\")\n",
    "    #image_path, appkey = sys.argv[1], sys.argv[2]\n",
    "    image_path, appkey = \"2.jpg\",\"0166edbccc8fae3043dc6f0830749d81\"\n",
    "\n",
    "    resize_impath = kakao_ocr_resize(image_path)\n",
    "    if resize_impath is not None:\n",
    "        image_path = resize_impath\n",
    "        print(\"원본 대신 리사이즈된 이미지를 사용합니다.\")\n",
    "\n",
    "    output = kakao_ocr_detect(image_path, appkey).json()\n",
    "    print(\"[detect] output:\\n{}\\n\".format(output))\n",
    "\n",
    "    boxes = output[\"result\"][\"boxes\"]\n",
    "    boxes = boxes[:min(len(boxes), LIMIT_BOX)]\n",
    "    output = kakao_ocr_recognize(image_path, boxes, appkey).json()\n",
    "    print(\"[recognize] output:\\n{}\\n\".format(json.dumps(output, sort_keys=True, indent=2)))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#카카오 개발자 \n",
    "https://developers.kakao.com/apps/393146/created"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
